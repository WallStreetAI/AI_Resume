{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOhtZvWOMviEFzuENRmErEb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamgagan/AI_Resume/blob/main/interview_prep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install frontend"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozcSrI8oAiYo",
        "outputId": "afa52ceb-0d07-4d7c-f2e9-254f80164256"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting frontend\n",
            "  Downloading frontend-0.0.3-py3-none-any.whl (32 kB)\n",
            "Collecting starlette>=0.12.0 (from frontend)\n",
            "  Downloading starlette-0.38.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.1/72.1 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn>=0.7.1 (from frontend)\n",
            "  Downloading uvicorn-0.30.3-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: itsdangerous>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from frontend) (2.2.0)\n",
            "Collecting aiofiles (from frontend)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette>=0.12.0->frontend) (3.7.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.7.1->frontend) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn>=0.7.1->frontend)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.7.1->frontend) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette>=0.12.0->frontend) (3.7)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette>=0.12.0->frontend) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette>=0.12.0->frontend) (1.2.2)\n",
            "Installing collected packages: h11, aiofiles, uvicorn, starlette, frontend\n",
            "Successfully installed aiofiles-24.1.0 frontend-0.0.3 h11-0.14.0 starlette-0.38.0 uvicorn-0.30.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wz_EQi8rApQY",
        "outputId": "0db764bb-6c8b-4098-8490-c80894c8073e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading PyMuPDF-1.24.7-cp310-none-manylinux2014_x86_64.whl (3.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyMuPDFb==1.24.6 (from PyMuPDF)\n",
            "  Downloading PyMuPDFb-1.24.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (15.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDFb, PyMuPDF\n",
            "Successfully installed PyMuPDF-1.24.7 PyMuPDFb-1.24.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kn3nJYZcAvu6",
        "outputId": "e65881d5-25d9-46b9-e12e-8a8e3b08ea25"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.36.0-py3-none-any.whl (328 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.7/328.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m614.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
            "Installing collected packages: httpcore, httpx, openai\n",
            "Successfully installed httpcore-1.0.5 httpx-0.27.0 openai-1.36.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWIJc_byA5Ns",
        "outputId": "c2714d37-af85-4112-cfa2-7d7fa795f65f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRdPsDlkCeQd",
        "outputId": "7543eb8c-f771-45a9-c5cb-456f12fd0e69"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PySimpleGUI"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iG-LRNSdAbSn",
        "outputId": "042e3189-7b5e-4014-dfd1-e2ce77f11242"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PySimpleGUI\n",
            "  Downloading PySimpleGUI-5.0.6-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: rsa in /usr/local/lib/python3.10/dist-packages (from PySimpleGUI) (4.9)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from rsa->PySimpleGUI) (0.6.0)\n",
            "Installing collected packages: PySimpleGUI\n",
            "Successfully installed PySimpleGUI-5.0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycq4ib5CGabo",
        "outputId": "4e556240-9367-4bfd-a3bd-461593004535"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-4.38.1-py3-none-any.whl (12.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting altair<6.0,>=5.0 (from gradio)\n",
            "  Downloading altair-5.3.0-py3-none-any.whl (857 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m857.8/857.8 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi (from gradio)\n",
            "  Downloading fastapi-0.111.1-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/92.2 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==1.1.0 (from gradio)\n",
            "  Downloading gradio_client-1.1.0-py3-none-any.whl (318 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.1/318.1 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.23.5)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.25.2)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.8.2)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.5.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.7)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.30.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.1.0->gradio) (2023.6.0)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==1.1.0->gradio)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=5.0->gradio) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=5.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.15.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.20.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n",
            "Collecting starlette<0.38.0,>=0.37.2 (from fastapi->gradio)\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi-cli>=0.0.2 (from fastapi->gradio)\n",
            "  Downloading fastapi_cli-0.0.4-py3-none-any.whl (9.5 kB)\n",
            "Collecting email_validator>=2.0.0 (from fastapi->gradio)\n",
            "  Downloading email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
            "Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi->gradio)\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=5.0->gradio) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=5.0->gradio) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=5.0->gradio) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=5.0->gradio) (0.19.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.16.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.24.1->gradio) (1.2.2)\n",
            "Collecting httptools>=0.5.0 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (1.0.1)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=63c2087831c33d5bef7ce4e61462071690d69347082653396602ce60be9d133f\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: pydub, ffmpy, websockets, uvloop, tomlkit, semantic-version, ruff, python-multipart, orjson, httptools, dnspython, aiofiles, watchfiles, starlette, email_validator, gradio-client, fastapi-cli, altair, fastapi, gradio\n",
            "  Attempting uninstall: aiofiles\n",
            "    Found existing installation: aiofiles 24.1.0\n",
            "    Uninstalling aiofiles-24.1.0:\n",
            "      Successfully uninstalled aiofiles-24.1.0\n",
            "  Attempting uninstall: starlette\n",
            "    Found existing installation: starlette 0.38.0\n",
            "    Uninstalling starlette-0.38.0:\n",
            "      Successfully uninstalled starlette-0.38.0\n",
            "  Attempting uninstall: altair\n",
            "    Found existing installation: altair 4.2.2\n",
            "    Uninstalling altair-4.2.2:\n",
            "      Successfully uninstalled altair-4.2.2\n",
            "Successfully installed aiofiles-23.2.1 altair-5.3.0 dnspython-2.6.1 email_validator-2.2.0 fastapi-0.111.1 fastapi-cli-0.0.4 ffmpy-0.3.2 gradio-4.38.1 gradio-client-1.1.0 httptools-0.6.1 orjson-3.10.6 pydub-0.25.1 python-multipart-0.0.9 ruff-0.5.4 semantic-version-2.10.0 starlette-0.37.2 tomlkit-0.12.0 uvloop-0.19.0 watchfiles-0.22.0 websockets-11.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5-doRSgAJRd",
        "outputId": "ce5ab3fc-caf9-411a-df7c-9b5a1dcaf5fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to the AI Interview Prep App!\n",
            "Enter the job position: Data Science Manager\n",
            "Enter the job description: About the job *Please note: This role will need to be physically located in our Woodbridge, NJ location.   The Data Science Manager will manage a team of statisticians in research and development of new predictive models and new analytical techniques related to insurance pure premiums, customer lifetime value, customer buying behaviors and related topics using internal and external data.   Essential Functions and Responsibilities  Provide analytics for setting price, underwriting and products strategies. Implement analysis and modeling tests. Develop and enhance models used in a variety of contexts within the product management structure. Advance modeling capabilities. Work with colleagues in various departments to facilitate the application of models developed considering the regulatory and competitive contexts. Ensure models are understood, implications are appropriately assessed, implementation plans are defined, validation of results are undertaken and adjustments are made. Supervise, manage and develop assigned staff   Qualifications and Education  Master's degree in Statistics, Mathematics, Engineering or related field (willing to accept foreign education equivalent) and a minimum of seven (7) years’ experience supervising predictive/advanced analytics. Alternatively, will accept Ph.D. and five years’ experience in supervising predictive/advanced analytics. Demonstrated expertise in SAS, R, or Python etc. Demonstrated expertise in Bayesian statistics. Demonstrated experience in developing multivariate predictive models in an insurance/financial fields.   About the Company  The Plymouth Rock Company and its affiliated group of companies write and manage over $2 billion in personal and commercial auto and homeowner’s insurance throughout the Northeast and mid-Atlantic, where we have built an unparalleled reputation for service. We continuously invest in technology, our employees thrive in our empowering environment, and our customers are among the most loyal in the industry. The Plymouth Rock group of companies employs more than 1,900 people and is headquartered in Boston, Massachusetts. Plymouth Rock Assurance Corporation holds an A.M. Best rating of “A-/Excellent”.\n",
            "Enter the path to your CV (PDF file): /content/Gagandeep Singh.pdf\n",
            "\n",
            "Great! Let's start the interview. Type 'quit' to end the session.\n",
            "\n",
            "\n",
            "Interviewer: Interview Question: \n",
            "Given your experience in developing multivariate predictive models in the insurance field, can you walk us through a specific project where you successfully collaborated with colleagues from various departments to ensure that the models developed were implemented considering regulatory and competitive contexts? How did you ensure that the models were well understood, implications were appropriately assessed, and adjustments were made as needed throughout the process?\n",
            "You: hi\n",
            "\n",
            "Evaluation: Score: 0/10\n",
            "\n",
            "Explanation: The candidate's response of \"hi\" does not address the interview question at all. It lacks any substance or relevant information related to the experience of collaborating on a project involving multivariate predictive models in the insurance field. This response demonstrates a complete failure to engage with the question and showcase the candidate's qualifications and experience.\n",
            "\n",
            "\n",
            "Interviewer: Interview Question:\n",
            "Given your extensive experience in leading data science teams and developing advanced models using cutting-edge technologies like GenAI, BERT, and GPT, can you share a specific example of a challenging technical problem you encountered in your role as a Data Science Manager? How did you approach solving this problem, what strategies did you implement, and what was the outcome in terms of model performance and business impact?\n",
            "You: As a Data Science Manager, I encountered a challenging problem when implementing a large-scale natural language processing model using BERT for customer sentiment analysis across multiple languages. To solve this, I led a cross-functional team to fine-tune the model on domain-specific data, implemented a distributed training pipeline to handle the massive dataset, and developed a custom evaluation framework to measure performance across languages, resulting in a 25% improvement in sentiment classification accuracy and enabling the company to respond more effectively to customer feedback in real-time across global markets.\n",
            "Warning: Could not extract score from evaluation. Setting score to 0.\n",
            "\n",
            "Evaluation: Score: 9\n",
            "\n",
            "Explanation:\n",
            "The candidate's response demonstrates their ability to effectively lead a cross-functional team in tackling a challenging technical problem related to natural language processing using BERT. They outlined clear strategies such as fine-tuning the model, implementing distributed training, and developing a custom evaluation framework, showcasing their technical expertise and problem-solving skills. The 25% improvement in sentiment classification accuracy and the impact on enabling real-time responses to customer feedback highlights the successful outcome of their approach. Overall, the response is detailed, relevant to the role of Data Science Manager, and effectively communicates the candidate's capabilities.\n",
            "\n",
            "\n",
            "Interviewer: Interview Question:\n",
            "Considering your experience in managing data science teams and developing advanced models, especially in the insurance domain, can you provide an example of a project where you had to balance the need for regulatory compliance with the aim of enhancing predictive modeling capabilities? How did you navigate this challenge, ensuring both regulatory requirements were met and modeling techniques were advanced effectively?\n",
            "You: quit\n",
            "\n",
            "Interview concluded. Your final score is: 0/10\n",
            "Thank you for using the AI Interview Prep App!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "from PyPDF2 import PdfReader\n",
        "from openai import OpenAI\n",
        "\n",
        "# Define your OpenAI API key here\n",
        "OPENAI_API_KEY = api_key\n",
        "\n",
        "class AIInterviewPrep:\n",
        "    def __init__(self):\n",
        "        self.job_position = \"\"\n",
        "        self.job_description = \"\"\n",
        "        self.candidate_cv = \"\"\n",
        "        self.interview_history = []\n",
        "        self.score = 0\n",
        "        self.client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "    def set_job_details(self, position, description):\n",
        "        self.job_position = position\n",
        "        self.job_description = description\n",
        "\n",
        "    def set_candidate_cv(self, cv_text):\n",
        "        self.candidate_cv = cv_text\n",
        "\n",
        "    def extract_text_from_pdf(self, file_path):\n",
        "        with open(file_path, 'rb') as file:\n",
        "            pdf = PdfReader(file)\n",
        "            text = \"\"\n",
        "            for page in pdf.pages:\n",
        "                text += page.extract_text()\n",
        "        return text\n",
        "\n",
        "    def generate_question(self):\n",
        "        prompt = f\"\"\"\n",
        "        Based on the following job position, description, and candidate CV, generate an interview question:\n",
        "        Job Position: {self.job_position}\n",
        "        Job Description: {self.job_description}\n",
        "        Candidate CV: {self.candidate_cv}\n",
        "        Interview History: {json.dumps(self.interview_history)}\n",
        "\n",
        "        Generate a relevant interview question, ranging from basic to advanced, that hasn't been asked before.\n",
        "        \"\"\"\n",
        "\n",
        "        response = self.client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an AI-powered interview assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        question = response.choices[0].message.content.strip()\n",
        "        self.interview_history.append({\"role\": \"interviewer\", \"content\": question})\n",
        "        return question\n",
        "\n",
        "    def evaluate_response(self, response):\n",
        "        prompt = f\"\"\"\n",
        "        Evaluate the following candidate response to the last interview question:\n",
        "        Job Position: {self.job_position}\n",
        "        Job Description: {self.job_description}\n",
        "        Candidate CV: {self.candidate_cv}\n",
        "        Last Question: {self.interview_history[-1]['content']}\n",
        "        Candidate Response: {response}\n",
        "\n",
        "        Provide a score from 0 to 10 and a brief explanation of the evaluation.\n",
        "        \"\"\"\n",
        "\n",
        "        evaluation = self.client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an AI-powered interview evaluator.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        eval_result = evaluation.choices[0].message.content.strip()\n",
        "\n",
        "        # Extract score using regex\n",
        "        score_match = re.search(r'(\\d+(?:\\.\\d+)?)/10', eval_result)\n",
        "        if score_match:\n",
        "            self.score = float(score_match.group(1))\n",
        "        else:\n",
        "            print(\"Warning: Could not extract score from evaluation. Setting score to 0.\")\n",
        "            self.score = 0\n",
        "\n",
        "        self.interview_history.append({\"role\": \"candidate\", \"content\": response})\n",
        "        self.interview_history.append({\"role\": \"evaluator\", \"content\": eval_result})\n",
        "        return eval_result\n",
        "\n",
        "    def run_interview(self):\n",
        "        print(\"Welcome to the AI Interview Prep App!\")\n",
        "        self.job_position = input(\"Enter the job position: \")\n",
        "        self.job_description = input(\"Enter the job description: \")\n",
        "        cv_path = input(\"Enter the path to your CV (PDF file): \")\n",
        "\n",
        "        if not os.path.exists(cv_path):\n",
        "            print(\"Error: The specified CV file does not exist.\")\n",
        "            return\n",
        "\n",
        "        self.candidate_cv = self.extract_text_from_pdf(cv_path)\n",
        "\n",
        "        print(\"\\nGreat! Let's start the interview. Type 'quit' to end the session.\\n\")\n",
        "\n",
        "        while True:\n",
        "            question = self.generate_question()\n",
        "            print(f\"\\nInterviewer: {question}\")\n",
        "\n",
        "            response = input(\"You: \")\n",
        "            if response.lower() == 'quit':\n",
        "                break\n",
        "\n",
        "            evaluation = self.evaluate_response(response)\n",
        "            print(f\"\\nEvaluation: {evaluation}\\n\")\n",
        "\n",
        "        print(f\"\\nInterview concluded. Your final score is: {self.score}/10\")\n",
        "        print(\"Thank you for using the AI Interview Prep App!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    interview_app = AIInterviewPrep()\n",
        "    interview_app.run_interview()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradio demo"
      ],
      "metadata": {
        "id": "YtS_Tq8GsIob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "from PyPDF2 import PdfReader\n",
        "from openai import OpenAI\n",
        "import gradio as gr\n",
        "\n",
        "# Define your OpenAI API key here\n",
        "OPENAI_API_KEY = api_key\n",
        "\n",
        "class AIInterviewPrep:\n",
        "    def __init__(self):\n",
        "        self.job_position = \"\"\n",
        "        self.job_description = \"\"\n",
        "        self.candidate_cv = \"\"\n",
        "        self.interview_history = []\n",
        "        self.score = 0\n",
        "        self.all_scores = []\n",
        "        self.all_feedback = []\n",
        "        self.client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "        self.current_question = \"\"\n",
        "\n",
        "    def set_job_details(self, position, description):\n",
        "        self.job_position = position\n",
        "        self.job_description = description\n",
        "\n",
        "    def set_candidate_cv(self, cv_text):\n",
        "        self.candidate_cv = cv_text\n",
        "\n",
        "    def extract_text_from_pdf(self, file):\n",
        "        pdf = PdfReader(file)\n",
        "        text = \"\"\n",
        "        for page in pdf.pages:\n",
        "            text += page.extract_text()\n",
        "        return text\n",
        "\n",
        "    def generate_question(self):\n",
        "        prompt = f\"\"\"\n",
        "        You are a human interviewer conducting a job interview for the position of {self.job_position}.\n",
        "        Based on the job description and the candidate's CV, as well as the interview history so far,\n",
        "        generate the next interview question. Your response should be in a natural, conversational tone\n",
        "        as if you're speaking directly to the candidate. You can reference previous answers if relevant.\n",
        "\n",
        "        Job Description: {self.job_description}\n",
        "        Candidate CV: {self.candidate_cv}\n",
        "        Interview History: {json.dumps(self.interview_history)}\n",
        "\n",
        "        Generate a relevant interview question that hasn't been asked before. The question should be specific\n",
        "        and require a detailed response. Start with a brief transitional phrase or acknowledgment of the previous\n",
        "        response before presenting the new question. Remember to speak as if you're having a real conversation.\n",
        "\n",
        "        Do not ask follow-up questions to the previous answer. Instead, move on to a new topic or aspect of the job.\n",
        "        \"\"\"\n",
        "\n",
        "        response = self.client.chat.completions.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a human interviewer. Respond in a natural, conversational manner.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.current_question = response.choices[0].message.content.strip()\n",
        "        self.interview_history.append({\"role\": \"interviewer\", \"content\": self.current_question})\n",
        "        return self.current_question\n",
        "\n",
        "    def evaluate_response(self, response):\n",
        "        if len(response.split()) < 5:\n",
        "            return self.generate_low_score_evaluation(response)\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        You are a human interviewer evaluating a candidate's response for the position of {self.job_position}.\n",
        "        Provide feedback on the candidate's answer in a natural, conversational tone. Your response should sound\n",
        "        like you're speaking directly to the candidate.\n",
        "\n",
        "        Job Description: {self.job_description}\n",
        "        Last Question: {self.current_question}\n",
        "        Candidate Response: {response}\n",
        "\n",
        "        Evaluate the response considering its relevance, depth, demonstration of skills, clarity, and alignment with job requirements.\n",
        "        Provide constructive feedback, highlighting strengths and suggesting areas for improvement.\n",
        "        Be encouraging but honest. Don't mention any numeric score in your response.\n",
        "\n",
        "        Your response should be structured as follows:\n",
        "        1. A brief acknowledgment of the candidate's response\n",
        "        2. Specific strengths of the response\n",
        "        3. Areas where the candidate could have expanded or improved their answer\n",
        "        4. A brief overall assessment\n",
        "        5. A smooth transition indicating that you're moving to the next question (but do not ask a new question)\n",
        "\n",
        "        Remember to maintain a conversational, human-like tone throughout your response. Do not include any follow-up questions.\n",
        "        \"\"\"\n",
        "\n",
        "        evaluation = self.client.chat.completions.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a human interviewer. Respond in a natural, conversational manner.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        eval_result = evaluation.choices[0].message.content.strip()\n",
        "\n",
        "        # For internal scoring (not shown to the candidate)\n",
        "        score_prompt = f\"Based on the evaluation you just gave, assign a score from 0 to 10 for this response. Only respond with the numeric score.\"\n",
        "        score_response = self.client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an AI assistant. Respond only with a numeric score.\"},\n",
        "                {\"role\": \"user\", \"content\": eval_result},\n",
        "                {\"role\": \"user\", \"content\": score_prompt}\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            self.score = float(score_response.choices[0].message.content.strip())\n",
        "        except ValueError:\n",
        "            print(\"Warning: Could not extract score from evaluation. Setting score to 0.\")\n",
        "            self.score = 0\n",
        "\n",
        "        self.all_scores.append(self.score)\n",
        "        self.all_feedback.append(eval_result)\n",
        "        self.interview_history.append({\"role\": \"candidate\", \"content\": response})\n",
        "        self.interview_history.append({\"role\": \"evaluator\", \"content\": eval_result})\n",
        "        return eval_result\n",
        "\n",
        "    def generate_low_score_evaluation(self, response):\n",
        "        evaluation = f\"\"\"\n",
        "        I appreciate your response, but I feel we could dive a bit deeper here. Could you elaborate more on your answer?\n",
        "        I'm really interested in hearing about your specific experiences and thoughts related to the question.\n",
        "        Don't be afraid to provide more details – this is your opportunity to showcase your skills and experience.\n",
        "        Let's move on to the next question, but please remember to provide more comprehensive answers going forward.\n",
        "        \"\"\"\n",
        "        self.score = 1 if len(response.split()) > 1 else 0\n",
        "        self.all_scores.append(self.score)\n",
        "        self.all_feedback.append(evaluation)\n",
        "        return evaluation\n",
        "\n",
        "interview_app = AIInterviewPrep()\n",
        "\n",
        "def start_interview(job_position, job_description, cv_file):\n",
        "    interview_app.set_job_details(job_position, job_description)\n",
        "    cv_text = interview_app.extract_text_from_pdf(cv_file)\n",
        "    interview_app.set_candidate_cv(cv_text)\n",
        "    question = interview_app.generate_question()\n",
        "    return question\n",
        "\n",
        "def answer_question(user_answer):\n",
        "    evaluation = interview_app.evaluate_response(user_answer)\n",
        "    next_question = interview_app.generate_question()\n",
        "    return f\"{evaluation}\\n\\nNext Question:\\n{next_question}\"\n",
        "\n",
        "def end_interview():\n",
        "    if len(interview_app.all_scores) > 0:\n",
        "        final_score = sum(interview_app.all_scores) / len(interview_app.all_scores)\n",
        "    else:\n",
        "        final_score = 0\n",
        "    num_questions = len(interview_app.all_scores)\n",
        "\n",
        "    # Generate overall feedback\n",
        "    overall_feedback = interview_app.client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a human interviewer providing final feedback. Respond in a natural, conversational manner.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Based on the following feedback from {num_questions} interview questions, provide an overall assessment of the candidate's performance, highlighting key strengths and areas for improvement. Speak directly to the candidate in a supportive and constructive manner:\\n\\n\" + \"\\n\\n\".join(interview_app.all_feedback)}\n",
        "        ]\n",
        "    ).choices[0].message.content.strip()\n",
        "\n",
        "    result = f\"Thank you for participating in this interview. Here's some overall feedback on your performance:\\n\\n{overall_feedback}\\n\\nYour average score across all questions was {final_score:.2f}/10.\"\n",
        "    interview_app.__init__()  # Reset the interview\n",
        "    return result\n",
        "\n",
        "# Gradio interface\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# AI Interview Prep App\")\n",
        "\n",
        "    with gr.Tab(\"Start Interview\"):\n",
        "        job_position = gr.Textbox(label=\"Job Position\")\n",
        "        job_description = gr.Textbox(label=\"Job Description\", lines=5)\n",
        "        cv_file = gr.File(label=\"Upload CV (PDF)\")\n",
        "        start_button = gr.Button(\"Start Interview\")\n",
        "        question_output = gr.Textbox(label=\"Interview Question\", lines=3)\n",
        "\n",
        "    with gr.Tab(\"Answer Questions\"):\n",
        "        user_answer = gr.Textbox(label=\"Your Answer\", lines=5)\n",
        "        submit_answer = gr.Button(\"Submit Answer\")\n",
        "        evaluation_output = gr.Textbox(label=\"Feedback and Next Question\", lines=10)\n",
        "\n",
        "    with gr.Tab(\"End Interview\"):\n",
        "        end_button = gr.Button(\"End Interview\")\n",
        "        final_score = gr.Textbox(label=\"Final Feedback\", lines=10)\n",
        "\n",
        "    start_button.click(start_interview, inputs=[job_position, job_description, cv_file], outputs=question_output)\n",
        "    submit_answer.click(answer_question, inputs=[user_answer], outputs=evaluation_output)\n",
        "    end_button.click(end_interview, outputs=final_score)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "eOJYmdWvGYsY",
        "outputId": "9fb06bb0-680e-4809-e4af-a577efec698a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://da1124a1e50a05595a.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://da1124a1e50a05595a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G7WDtwP2HCbC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}